{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KDM_ICP5.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OUEr8ztPfwz",
        "outputId": "e65eca08-da9d-4c8f-f7d6-416b883158cc"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/67/5158f846202d7f012d1c9ca21c3549a58fd3c6707ae8ee823adcaca6473c/pyspark-3.0.2.tar.gz (204.8MB)\n",
            "\u001b[K     |████████████████████████████████| 204.8MB 77kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 19.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.2-py2.py3-none-any.whl size=205186687 sha256=30fef9f2c826a02610d4e72e2e8e6cad9f110be6d5ca37e9b8805634b55a41a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/09/da/c1f2859bcc86375dc972c5b6af4881b3603269bcc4c9be5d16\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gFSCm5PPf96"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "from pyspark import SparkConf, SparkContext\r\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.ml.feature import NGram\r\n",
        "from pyspark.ml.feature import Word2Vec"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3RqI5W9PwEU"
      },
      "source": [
        "# Create the Spark session\r\n",
        "spark = SparkSession.builder.appName(\"TF_IDF\").getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ynmbdljP37e"
      },
      "source": [
        "# Create the dataframe with five text abstracts\r\n",
        "abst = spark.read.text('input*.txt')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAuDw1JTP_j9"
      },
      "source": [
        "# Tokenize the abstract texts\r\n",
        "tokenizer = Tokenizer(inputCol=\"value\", outputCol=\"words\")\r\n",
        "wordsData = tokenizer.transform(abst)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juPnzf1HkBNi",
        "outputId": "9fae5ad0-6a8c-4eb0-b876-baa608fb23b9"
      },
      "source": [
        "wordsData.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|               value|               words|\n",
            "+--------------------+--------------------+\n",
            "|The term soul is ...|[the, term, soul,...|\n",
            "|subjective essenc...|[subjective, esse...|\n",
            "|from its investig...|[from, its, inves...|\n",
            "|literature and ex...|[literature, and,...|\n",
            "|their study is to...|[their, study, is...|\n",
            "|literature. In th...|[literature., in,...|\n",
            "|quantum physics, ...|[quantum, physics...|\n",
            "|studies on medita...|[studies, on, med...|\n",
            "|context, this pap...|[context,, this, ...|\n",
            "|as uninhibited me...|[as, uninhibited,...|\n",
            "|Among several pos...|[among, several, ...|\n",
            "|provided by theor...|[provided, by, th...|\n",
            "|theory. We show m...|[theory., we, sho...|\n",
            "|inhabitants of a ...|[inhabitants, of,...|\n",
            "|accurately measur...|[accurately, meas...|\n",
            "|they cannot relia...|[they, cannot, re...|\n",
            "|factual questions...|[factual, questio...|\n",
            "|with those in oth...|[with, those, in,...|\n",
            "|apply to observer...|[apply, to, obser...|\n",
            "|            theory).|          [theory).]|\n",
            "+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWWoLXIKQEp-"
      },
      "source": [
        "# applying tf on the words data\r\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10)\r\n",
        "featurizedData = hashingTF.transform(wordsData)\r\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqsyc3TFQLX4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf8ad8e5-7bb1-48a5-86a7-7e1b50b06829"
      },
      "source": [
        "# calculating the IDF\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idfModel = idf.fit(featurizedData)\r\n",
        "rescaledData = idfModel.transform(featurizedData)\r\n",
        "print(rescaledData)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataFrame[value: string, words: array<string>, rawFeatures: vector, features: vector]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ej95DLvjQS1T",
        "outputId": "d387c723-a476-483b-cd1c-19027f600d2d"
      },
      "source": [
        "# Display the results\r\n",
        "rescaledData.select(\"features\").show(10, truncate=False)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|features                                                                                                                                                                                                                    |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|(10,[0,1,3,4,5,6,7,8,9],[0.4289956055183586,0.5293851084541643,0.35536235447490483,0.2954642128938359,0.17768117723745241,0.7940776626812465,0.19051219595701865,0.41170410840829763,1.174197955387005])                    |\n",
            "|(10,[1,2,3,4,5,6,7,8,9],[0.5293851084541643,1.0062071553441607,0.5330435317123572,0.2954642128938359,0.35536235447490483,0.5293851084541643,0.09525609797850933,0.8234082168165953,0.7045187732322029])                     |\n",
            "|(10,[0,1,2,3,6,7,8,9],[0.4289956055183586,0.7940776626812465,0.5031035776720804,0.35536235447490483,0.26469255422708216,0.19051219595701865,1.0292602710207441,0.469679182154802])                                          |\n",
            "|(10,[0,1,3,4,5,7,9],[0.4289956055183586,0.5293851084541643,0.35536235447490483,0.5909284257876718,0.7107247089498097,0.19051219595701865,0.7045187732322029])                                                               |\n",
            "|(10,[0,1,3,4,5,6,7,8,9],[0.4289956055183586,0.5293851084541643,0.5330435317123572,0.2954642128938359,0.17768117723745241,0.26469255422708216,0.2381402449462733,0.20585205420414882,1.174197955387005])                     |\n",
            "|(10,[0,1,2,3,5,7,8,9],[1.2869868165550757,0.5293851084541643,1.5093107330162412,0.35536235447490483,0.17768117723745241,0.19051219595701865,0.6175561626124464,0.469679182154802])                                          |\n",
            "|(10,[1,2,3,4,5,6,7,8,9],[0.26469255422708216,1.0062071553441607,0.7107247089498097,0.2954642128938359,0.35536235447490483,0.7940776626812465,0.09525609797850933,0.6175561626124464,0.234839591077401])                     |\n",
            "|(10,[1,2,3,4,5,6,7,8,9],[0.5293851084541643,0.5031035776720804,0.7107247089498097,0.5909284257876718,0.5330435317123572,0.26469255422708216,0.142884146967764,0.20585205420414882,0.234839591077401])                       |\n",
            "|(10,[0,1,2,3,4,5,6,7,8,9],[0.8579912110367172,0.26469255422708216,0.5031035776720804,0.17768117723745241,0.5909284257876718,0.5330435317123572,0.5293851084541643,0.09525609797850933,0.6175561626124464,0.939358364309604])|\n",
            "|(10,[1,7,8,9],[0.26469255422708216,0.047628048989254664,0.20585205420414882,0.234839591077401])                                                                                                                             |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pay6pKJckwUe"
      },
      "source": [
        "# Create the Spark session\r\n",
        "spark2 = SparkSession.builder.appName(\"Ngrams\").getOrCreate()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2_ZUrdKkxZW"
      },
      "source": [
        "# Create the dataframe with five text abstracts\r\n",
        "abst = spark2.read.text('input*.txt')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9X0AOJNk4Vj"
      },
      "source": [
        "# Tokenize the abstract texts\r\n",
        "tokenizer = Tokenizer(inputCol=\"value\", outputCol=\"words\")\r\n",
        "wordsData = tokenizer.transform(abst)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAzcDVkOk7ky"
      },
      "source": [
        "\r\n",
        "# Creating n-grams with n=5\r\n",
        "ngram = NGram(n=5, inputCol=\"words\", outputCol=\"ngrams\")\r\n",
        "ngramDataFrame = ngram.transform(wordsData)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0DP-h8yk-X0"
      },
      "source": [
        "# Apply topic frequency on the abstracts\r\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10)\r\n",
        "featurizedData = hashingTF.transform(ngramDataFrame)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz3u0f3NlEDQ"
      },
      "source": [
        "# Calculate the inverse document frequency\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idfModel = idf.fit(featurizedData)\r\n",
        "rescaledData = idfModel.transform(featurizedData)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqmrnCbnlREk",
        "outputId": "4f75e6dd-4847-4015-ff29-57c4a305f631"
      },
      "source": [
        "# Display the results\r\n",
        "rescaledData.select(\"features\").show(10, truncate=False)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|features                                                                                                                                                                                                                    |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|(10,[0,1,3,4,5,6,7,8,9],[0.4289956055183586,0.5293851084541643,0.35536235447490483,0.2954642128938359,0.17768117723745241,0.7940776626812465,0.19051219595701865,0.41170410840829763,1.174197955387005])                    |\n",
            "|(10,[1,2,3,4,5,6,7,8,9],[0.5293851084541643,1.0062071553441607,0.5330435317123572,0.2954642128938359,0.35536235447490483,0.5293851084541643,0.09525609797850933,0.8234082168165953,0.7045187732322029])                     |\n",
            "|(10,[0,1,2,3,6,7,8,9],[0.4289956055183586,0.7940776626812465,0.5031035776720804,0.35536235447490483,0.26469255422708216,0.19051219595701865,1.0292602710207441,0.469679182154802])                                          |\n",
            "|(10,[0,1,3,4,5,7,9],[0.4289956055183586,0.5293851084541643,0.35536235447490483,0.5909284257876718,0.7107247089498097,0.19051219595701865,0.7045187732322029])                                                               |\n",
            "|(10,[0,1,3,4,5,6,7,8,9],[0.4289956055183586,0.5293851084541643,0.5330435317123572,0.2954642128938359,0.17768117723745241,0.26469255422708216,0.2381402449462733,0.20585205420414882,1.174197955387005])                     |\n",
            "|(10,[0,1,2,3,5,7,8,9],[1.2869868165550757,0.5293851084541643,1.5093107330162412,0.35536235447490483,0.17768117723745241,0.19051219595701865,0.6175561626124464,0.469679182154802])                                          |\n",
            "|(10,[1,2,3,4,5,6,7,8,9],[0.26469255422708216,1.0062071553441607,0.7107247089498097,0.2954642128938359,0.35536235447490483,0.7940776626812465,0.09525609797850933,0.6175561626124464,0.234839591077401])                     |\n",
            "|(10,[1,2,3,4,5,6,7,8,9],[0.5293851084541643,0.5031035776720804,0.7107247089498097,0.5909284257876718,0.5330435317123572,0.26469255422708216,0.142884146967764,0.20585205420414882,0.234839591077401])                       |\n",
            "|(10,[0,1,2,3,4,5,6,7,8,9],[0.8579912110367172,0.26469255422708216,0.5031035776720804,0.17768117723745241,0.5909284257876718,0.5330435317123572,0.5293851084541643,0.09525609797850933,0.6175561626124464,0.939358364309604])|\n",
            "|(10,[1,7,8,9],[0.26469255422708216,0.047628048989254664,0.20585205420414882,0.234839591077401])                                                                                                                             |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFA_HEDXbcZL"
      },
      "source": [
        "# Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFY0atcwQiCs"
      },
      "source": [
        "spark3 = SparkSession.builder.appName(\"Ngram Similarity\").getOrCreate()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j1F-kR6Q2Ay"
      },
      "source": [
        "# Create the dataframe with five text abstracts\r\n",
        "abst = spark3.read.text('input*.txt')\r\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLrbsZ_2Z-0j"
      },
      "source": [
        "# Tokenize the abstract texts\r\n",
        "tokenizer = Tokenizer(inputCol=\"value\", outputCol=\"words\")\r\n",
        "wordsData = tokenizer.transform(abst)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xnz2_iOQ_xO"
      },
      "source": [
        "# Creating n-grams with n=2\r\n",
        "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\r\n",
        "ngramDataFrame = ngram.transform(wordsData)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMhJK57bRGD5",
        "outputId": "4af9ef0a-d010-4edc-d3f6-e78bd0835a0c"
      },
      "source": [
        "# displaying the results\r\n",
        "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|ngrams                                                                                                                                                                                                                                                  |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[the term, term soul, soul is, is used, used in, in the, the traditional, traditional literature, literature as, as a, a synonym, synonym for, for one’s, one’s true, true self, self and, and is, is associated, associated with, with the]            |\n",
            "|[subjective essence, essence of, of one’s, one’s living., living. since,, since, we, we don’t, don’t have, have any, any means, means to, to quantify, quantify it,, it, the, the science, science has, has ruled, ruled out, out this, this idea]      |\n",
            "|[from its, its investigations., investigations. but,, but, in, in a, a recent, recent study,, study, ceylan, ceylan et, et al., al. (2017), (2017) has, has reintroduced, reintroduced the, the word, word soul, soul to, to scientific]                |\n",
            "|[literature and, and examined, examined the, the possibility, possibility of, of the, the study, study of, of the, the soul, soul through, through scientific, scientific modalities., modalities. the, the primary, primary focus, focus of]           |\n",
            "|[their study, study is, is to, to find, find and, and understand, understand the, the scientific, scientific analog, analog of, of the, the soul, soul as, as quoted, quoted and, and discussed, discussed in, in the, the traditional]                 |\n",
            "|[literature. in, in the, the present, present paper,, paper, we, we examine, examine the, the idea, idea of, of a, a soul, soul that, that uses, uses a, a novel, novel approach;, approach; integrating, integrating neuroscience, neuroscience and]   |\n",
            "|[quantum physics,, physics, as, as proposed, proposed in, in ceylan, ceylan et, et al., al. (2017)., (2017). for, for this, this purpose,, purpose, we, we make, make use, use of, of findings, findings from, from neuroscientific]                    |\n",
            "|[studies on, on meditation, meditation to, to understand, understand the, the concepts, concepts of, of soul, soul and, and consciousness, consciousness in, in terms, terms of, of inhibition, inhibition mechanisms., mechanisms. in, in this]        |\n",
            "|[context, this, this paper, paper serves, serves as, as an, an attempt, attempt to, to call, call for, for more, more studies, studies to, to discuss, discuss and, and expand, expand the, the hypothesis, hypothesis about, about the, the soul]      |\n",
            "|[as uninhibited, uninhibited mental, mental activity.]                                                                                                                                                                                                  |\n",
            "|[among several, several possibilities, possibilities for, for what, what reality, reality could, could be, be like, like in, in view, view of, of the, the empirical, empirical facts, facts of, of quantum, quantum mechanics,, mechanics, one, one is]|\n",
            "|[provided by, by theories, theories of, of spontaneous, spontaneous wave, wave function, function collapse,, collapse, the, the best, best known, known of, of which, which is, is the, the ghirardi–rimini–weber, ghirardi–rimini–weber (grw)]         |\n",
            "|[theory. we, we show, show mathematically, mathematically that, that in, in grw, grw theory, theory (and, (and similar, similar theories), theories) there, there are, are limitations, limitations to, to knowledge,, knowledge, that, that is,]       |\n",
            "|[inhabitants of, of a, a grw, grw universe, universe cannot, cannot find, find out, out all, all the, the facts, facts true, true of, of their, their universe., universe. as, as a, a specific, specific example,, example, they, they cannot]         |\n",
            "|[accurately measure, measure the, the number, number of, of collapses, collapses that, that a, a given, given physical, physical system, system undergoes, undergoes during, during a, a given, given time, time interval;, interval; in, in fact,]     |\n",
            "|[they cannot, cannot reliably, reliably measure, measure whether, whether one, one or, or zero, zero collapses, collapses occur., occur. put, put differently,, differently, in, in a, a grw, grw universe, universe certain, certain meaningful,]      |\n",
            "|[factual questions, questions are, are empirically, empirically undecidable., undecidable. we, we discuss, discuss several, several types, types of, of limitations, limitations to, to knowledge, knowledge and, and compare, compare them]            |\n",
            "|[with those, those in, in other, other (no-collapse), (no-collapse) versions, versions of, of quantum, quantum mechanics,, mechanics, such, such as, as bohmian, bohmian mechanics., mechanics. most, most of, of our, our results, results also]       |\n",
            "|[apply to, to observer-induced, observer-induced collapses, collapses as, as in, in orthodox, orthodox quantum, quantum mechanics, mechanics (as, (as opposed, opposed to, to the, the spontaneous, spontaneous collapses, collapses of, of grw]        |\n",
            "|[]                                                                                                                                                                                                                                                      |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbsTbASRZN7V",
        "outputId": "6d217c43-7628-42ab-f731-a5f594b5975f"
      },
      "source": [
        "# Create a mapping from words to vectors\r\n",
        "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"words\", outputCol=\"result\")\r\n",
        "model = word2Vec.fit(ngramDataFrame)\r\n",
        "print(model.getVectors().collect())\r\n",
        "result = model.getVectors().collect()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Row(word='beneath', vector=DenseVector([-0.063, -0.1433, -0.0275])), Row(word='used', vector=DenseVector([0.1425, 0.0271, 0.1235])), Row(word='providing', vector=DenseVector([0.0157, 0.098, -0.1096])), Row(word='cells', vector=DenseVector([-0.0462, -0.0827, -0.1413])), Row(word='measure', vector=DenseVector([0.0346, 0.0103, 0.1353])), Row(word='is,', vector=DenseVector([-0.0772, -0.1257, 0.1589])), Row(word='number', vector=DenseVector([-0.1333, -0.0183, 0.1237])), Row(word='for', vector=DenseVector([-0.0761, 0.1601, -0.0055])), Row(word='hamiltonian', vector=DenseVector([0.1538, 0.0424, -0.0704])), Row(word='find', vector=DenseVector([0.1352, -0.1649, 0.1625])), Row(word='factual', vector=DenseVector([0.1439, 0.1665, 0.1454])), Row(word='superconscious', vector=DenseVector([-0.0677, -0.011, 0.0132])), Row(word='shifts', vector=DenseVector([-0.0902, -0.1046, -0.1303])), Row(word='due', vector=DenseVector([0.1268, 0.0562, 0.0994])), Row(word='proposed', vector=DenseVector([-0.0757, -0.0869, -0.1081])), Row(word='any', vector=DenseVector([0.0894, 0.001, 0.0699])), Row(word='lead', vector=DenseVector([-0.1504, 0.1371, 0.0823])), Row(word='undecidable.', vector=DenseVector([0.1591, 0.0633, 0.034])), Row(word='experiments', vector=DenseVector([0.1016, 0.1439, -0.0483])), Row(word='approach;', vector=DenseVector([-0.0807, -0.0211, 0.0165])), Row(word='operations', vector=DenseVector([0.0689, -0.0238, -0.0536])), Row(word='this', vector=DenseVector([0.0381, -0.1084, 0.0145])), Row(word='in', vector=DenseVector([0.1401, -0.1509, -0.0453])), Row(word='recombination', vector=DenseVector([0.0159, 0.0158, 0.1202])), Row(word='inspired', vector=DenseVector([-0.1583, 0.1131, 0.0766])), Row(word='provided', vector=DenseVector([0.0632, -0.164, -0.0283])), Row(word='attention,', vector=DenseVector([0.1613, -0.1311, -0.1568])), Row(word='have', vector=DenseVector([0.0877, -0.0735, 0.1397])), Row(word='point', vector=DenseVector([-0.0249, 0.0218, -0.0449])), Row(word='are', vector=DenseVector([-0.1208, -0.0561, 0.0796])), Row(word='is', vector=DenseVector([0.0292, -0.0396, 0.1132])), Row(word='absence', vector=DenseVector([-0.0978, -0.0825, -0.0711])), Row(word='claim', vector=DenseVector([0.0149, 0.157, 0.0927])), Row(word='among', vector=DenseVector([0.0465, 0.0817, 0.1324])), Row(word='governs', vector=DenseVector([-0.0087, -0.0889, -0.0801])), Row(word='force', vector=DenseVector([0.0349, -0.0802, 0.0697])), Row(word='show', vector=DenseVector([0.0741, -0.1623, 0.1442])), Row(word='grw', vector=DenseVector([0.1549, -0.1376, 0.029])), Row(word='independent', vector=DenseVector([0.1453, 0.0152, -0.0456])), Row(word='but,', vector=DenseVector([-0.0381, -0.1197, 0.0518])), Row(word='given', vector=DenseVector([0.1065, 0.1558, -0.0968])), Row(word='hence', vector=DenseVector([0.1554, -0.1071, 0.0683])), Row(word='system', vector=DenseVector([0.1723, -0.015, -0.1151])), Row(word='scientific', vector=DenseVector([0.0007, -0.0285, 0.1615])), Row(word='prior', vector=DenseVector([-0.1208, 0.1237, -0.1534])), Row(word='examined', vector=DenseVector([-0.1408, -0.0718, 0.1125])), Row(word='interaction', vector=DenseVector([-0.0506, -0.008, -0.0731])), Row(word='model', vector=DenseVector([-0.0822, -0.0863, -0.1285])), Row(word='empirical', vector=DenseVector([0.1556, -0.1035, 0.1311])), Row(word='observer-induced', vector=DenseVector([-0.0024, -0.0321, -0.1496])), Row(word='but', vector=DenseVector([-0.1282, 0.1486, -0.0047])), Row(word='analyse', vector=DenseVector([-0.1486, 0.1636, -0.0636])), Row(word='whether', vector=DenseVector([-0.0425, -0.0214, -0.0652])), Row(word='optimize', vector=DenseVector([-0.1651, 0.0628, 0.0488])), Row(word='physics', vector=DenseVector([-0.1613, 0.084, -0.0335])), Row(word='quantify', vector=DenseVector([-0.0957, 0.0409, -0.1418])), Row(word='potentials', vector=DenseVector([0.0204, -0.0481, 0.0783])), Row(word='best', vector=DenseVector([-0.0984, -0.1505, 0.0998])), Row(word='subjective', vector=DenseVector([-0.0164, 0.1176, 0.1191])), Row(word='known.', vector=DenseVector([-0.0652, 0.0367, 0.0369])), Row(word='collapse,', vector=DenseVector([-0.1485, -0.112, -0.1492])), Row(word='essential', vector=DenseVector([-0.0724, -0.1233, -0.0902])), Row(word='similar', vector=DenseVector([0.1362, 0.0019, 0.0223])), Row(word='trajectories,', vector=DenseVector([0.1459, -0.0201, 0.0077])), Row(word='what', vector=DenseVector([-0.0051, 0.1525, -0.1614])), Row(word='(and', vector=DenseVector([0.0979, 0.038, -0.0964])), Row(word='recent', vector=DenseVector([0.0095, -0.0438, 0.076])), Row(word='hypothesis', vector=DenseVector([-0.1676, 0.0467, 0.121])), Row(word='problem', vector=DenseVector([0.1003, 0.0472, -0.0611])), Row(word='versions', vector=DenseVector([-0.1129, 0.1624, -0.0347])), Row(word='theory.', vector=DenseVector([-0.0157, -0.0651, -0.0112])), Row(word='up', vector=DenseVector([0.0919, 0.077, -0.1017])), Row(word='going', vector=DenseVector([0.0082, -0.1061, 0.1008])), Row(word='uses', vector=DenseVector([0.1294, -0.0754, -0.1051])), Row(word='our', vector=DenseVector([-0.1131, 0.1066, 0.1145])), Row(word='zero', vector=DenseVector([0.068, -0.0965, 0.0174])), Row(word='considers', vector=DenseVector([0.0592, -0.1116, -0.1301])), Row(word='play', vector=DenseVector([-0.0919, -0.1508, 0.0])), Row(word='purpose,', vector=DenseVector([0.0954, 0.0607, 0.1222])), Row(word='electron–hole', vector=DenseVector([-0.1294, 0.1456, 0.0387])), Row(word='all', vector=DenseVector([-0.0484, -0.0223, -0.0632])), Row(word='studies', vector=DenseVector([0.0452, -0.0635, 0.0277])), Row(word='theory).', vector=DenseVector([0.07, 0.1051, -0.0922])), Row(word='present', vector=DenseVector([0.0843, 0.0406, -0.1225])), Row(word='facts', vector=DenseVector([-0.1033, 0.033, 0.0358])), Row(word='integrating', vector=DenseVector([0.0451, 0.0838, 0.0343])), Row(word='consciousness', vector=DenseVector([-0.1379, 0.1194, 0.076])), Row(word='idea', vector=DenseVector([-0.1351, 0.1122, -0.0277])), Row(word='equations', vector=DenseVector([0.1655, 0.0413, 0.0979])), Row(word='medium,', vector=DenseVector([0.1364, 0.1519, -0.0108])), Row(word='examine', vector=DenseVector([0.1034, -0.042, 0.1071])), Row(word='one’s', vector=DenseVector([-0.0092, -0.1152, -0.1395])), Row(word='possibilities', vector=DenseVector([-0.094, -0.1277, 0.1646])), Row(word='us', vector=DenseVector([0.0584, 0.1205, -0.0384])), Row(word='understand', vector=DenseVector([0.0186, 0.1265, -0.0556])), Row(word='it', vector=DenseVector([-0.1437, 0.0743, -0.0463])), Row(word='case.', vector=DenseVector([-0.1033, 0.0861, -0.166])), Row(word='reintroduced', vector=DenseVector([-0.0783, 0.1076, -0.0108])), Row(word='impact', vector=DenseVector([-0.1308, -0.141, 0.1115])), Row(word='soul', vector=DenseVector([-0.0991, 0.003, -0.0161])), Row(word='separation', vector=DenseVector([0.0369, 0.0492, -0.007])), Row(word='literature', vector=DenseVector([-0.1382, 0.049, -0.0637])), Row(word='self', vector=DenseVector([-0.1072, -0.164, 0.0805])), Row(word='laws', vector=DenseVector([0.0295, -0.1273, 0.1156])), Row(word='term', vector=DenseVector([-0.1625, 0.1502, -0.1045])), Row(word='a', vector=DenseVector([-0.1126, -0.1504, -0.112])), Row(word='tight-binding', vector=DenseVector([-0.1216, -0.165, -0.0008])), Row(word='moving', vector=DenseVector([0.1617, 0.0573, 0.008])), Row(word='study,', vector=DenseVector([-0.0007, -0.0326, 0.1583])), Row(word='wave-like', vector=DenseVector([-0.1672, 0.0811, 0.0675])), Row(word='don’t', vector=DenseVector([-0.0837, 0.0842, -0.0558])), Row(word='receive', vector=DenseVector([0.0288, -0.1249, -0.1204])), Row(word='attention', vector=DenseVector([0.1481, -0.0166, -0.0295])), Row(word='field,', vector=DenseVector([-0.0548, 0.0885, -0.0551])), Row(word='within', vector=DenseVector([-0.1283, 0.1032, 0.0145])), Row(word='as', vector=DenseVector([0.1441, 0.0661, 0.0266])), Row(word='yield', vector=DenseVector([0.0884, 0.1056, -0.1474])), Row(word='knowledge,', vector=DenseVector([0.0951, -0.0455, -0.0339])), Row(word='has', vector=DenseVector([-0.085, -0.0409, 0.0694])), Row(word='opposed', vector=DenseVector([0.07, -0.0857, -0.0557])), Row(word='demonstrate', vector=DenseVector([0.0164, 0.0097, 0.1619])), Row(word='theories', vector=DenseVector([-0.1497, 0.0745, 0.0622])), Row(word='moreover,', vector=DenseVector([0.0523, -0.1041, 0.1516])), Row(word='trajectories', vector=DenseVector([-0.0791, -0.0489, 0.009])), Row(word='bohmian', vector=DenseVector([-0.0416, -0.1232, 0.016])), Row(word='certain', vector=DenseVector([0.1368, -0.1005, -0.0092])), Row(word='mechanics', vector=DenseVector([-0.0909, -0.0208, 0.136])), Row(word='footing', vector=DenseVector([0.1023, 0.0187, 0.0861])), Row(word='optics', vector=DenseVector([0.087, -0.1525, -0.1648])), Row(word='concepts', vector=DenseVector([-0.0431, 0.1673, 0.1063])), Row(word='numerical', vector=DenseVector([0.109, -0.0642, -0.0913])), Row(word='or', vector=DenseVector([0.0114, 0.0634, 0.0905])), Row(word='describe', vector=DenseVector([0.1166, -0.131, -0.0902])), Row(word='mechanics.', vector=DenseVector([-0.0349, 0.0511, -0.0893])), Row(word='coupling,', vector=DenseVector([-0.0341, -0.0265, -0.0282])), Row(word='processing', vector=DenseVector([-0.0142, -0.0018, 0.0305])), Row(word='questions', vector=DenseVector([-0.0053, -0.0637, -0.0932])), Row(word='come', vector=DenseVector([-0.1758, 0.1491, 0.1211])), Row(word='such', vector=DenseVector([-0.0321, -0.0646, 0.0265])), Row(word='they', vector=DenseVector([-0.0381, -0.0613, 0.131])), Row(word='context,', vector=DenseVector([-0.0005, 0.0038, -0.0816])), Row(word='activity.', vector=DenseVector([0.0549, -0.0118, 0.0414])), Row(word='et', vector=DenseVector([0.0436, 0.0879, -0.0078])), Row(word='limiting', vector=DenseVector([0.0806, 0.1472, 0.1377])), Row(word='decision', vector=DenseVector([-0.0841, 0.0035, -0.0492])), Row(word='mathematics', vector=DenseVector([-0.0071, -0.0742, 0.0714])), Row(word='optical', vector=DenseVector([0.0193, -0.1026, -0.1307])), Row(word='(including', vector=DenseVector([0.0771, 0.1093, -0.0207])), Row(word='serves', vector=DenseVector([0.0299, -0.0737, 0.1538])), Row(word='inexpensive', vector=DenseVector([0.1082, 0.1081, -0.1089])), Row(word='collective', vector=DenseVector([-0.1355, 0.0153, 0.1578])), Row(word='conscious', vector=DenseVector([-0.107, 0.0504, -0.0508])), Row(word='solar', vector=DenseVector([0.0095, 0.1189, -0.0511])), Row(word='subtly', vector=DenseVector([0.0299, -0.0339, 0.1162])), Row(word='it,', vector=DenseVector([0.1554, -0.0145, 0.1659])), Row(word='synonym', vector=DenseVector([-0.0488, -0.1408, -0.0964])), Row(word='(as', vector=DenseVector([-0.0123, 0.0917, 0.1372])), Row(word='that', vector=DenseVector([0.0014, -0.1072, 0.1209])), Row(word='out', vector=DenseVector([0.1495, -0.1198, 0.1084])), Row(word='cannot', vector=DenseVector([-0.0214, -0.098, 0.0127])), Row(word='(2017).', vector=DenseVector([0.0673, -0.1077, -0.0588])), Row(word='diffraction', vector=DenseVector([-0.1147, 0.0649, 0.1074])), Row(word='possibility', vector=DenseVector([-0.0353, 0.1422, -0.1081])), Row(word='to', vector=DenseVector([-0.2162, -0.0654, 0.0113])), Row(word='science', vector=DenseVector([-0.0855, 0.0865, -0.0924])), Row(word='direct', vector=DenseVector([-0.027, -0.1645, 0.0177])), Row(word='exact', vector=DenseVector([-0.0609, 0.145, -0.0603])), Row(word='expand', vector=DenseVector([-0.0853, -0.1053, 0.0741])), Row(word='helmholtz', vector=DenseVector([0.0583, 0.0339, -0.1309])), Row(word='dynamical', vector=DenseVector([-0.0942, 0.1368, 0.1364])), Row(word='associated', vector=DenseVector([-0.1156, 0.0113, -0.1051])), Row(word='interference)', vector=DenseVector([-0.094, 0.0688, 0.0007])), Row(word='traditional', vector=DenseVector([0.1156, 0.0889, -0.1072])), Row(word='findings', vector=DenseVector([-0.0097, -0.0067, 0.1007])), Row(word='fact', vector=DenseVector([0.0107, 0.0254, -0.0428])), Row(word='organic', vector=DenseVector([0.1672, 0.1412, -0.0377])), Row(word='carriers.', vector=DenseVector([-0.1265, -0.1093, 0.1384])), Row(word='awareness', vector=DenseVector([-0.0303, -0.1291, 0.0837])), Row(word='most', vector=DenseVector([0.1448, 0.0278, -0.1646])), Row(word='here', vector=DenseVector([0.0158, -0.156, 0.1465])), Row(word='neuroscience', vector=DenseVector([0.135, -0.0921, 0.1461])), Row(word='(no-collapse)', vector=DenseVector([0.0073, 0.1175, 0.1163])), Row(word='help', vector=DenseVector([-0.0091, 0.1435, 0.0235])), Row(word='discussed', vector=DenseVector([0.0791, 0.1173, 0.0563])), Row(word='there', vector=DenseVector([0.1167, -0.0809, -0.0803])), Row(word='reliably', vector=DenseVector([0.0801, -0.0693, 0.0036])), Row(word='stationary', vector=DenseVector([-0.0935, -0.1111, 0.0376])), Row(word='discuss', vector=DenseVector([-0.065, 0.0626, -0.0141])), Row(word='meditation', vector=DenseVector([0.0892, 0.1143, 0.1154])), Row(word='through', vector=DenseVector([0.0677, 0.0851, -0.1395])), Row(word='points', vector=DenseVector([-0.1749, -0.0689, 0.09])), Row(word='readiness', vector=DenseVector([-0.0141, 0.1588, 0.1519])), Row(word='mechanisms.', vector=DenseVector([0.145, 0.1421, 0.1132])), Row(word='leads', vector=DenseVector([0.0509, 0.1065, -0.1309])), Row(word='universe.', vector=DenseVector([-0.0789, -0.0451, -0.1055])), Row(word='results', vector=DenseVector([-0.1495, 0.1244, -0.0144])), Row(word='wave', vector=DenseVector([-0.1481, -0.0413, 0.1665])), Row(word='limit.', vector=DenseVector([0.1466, -0.1346, 0.1383])), Row(word='subconscious', vector=DenseVector([0.0172, 0.0271, -0.1555])), Row(word='also', vector=DenseVector([0.1659, -0.165, -0.0482])), Row(word='processes', vector=DenseVector([-0.075, -0.0939, 0.0352])), Row(word='on', vector=DenseVector([-0.0022, -0.0945, 0.0283])), Row(word='method,', vector=DenseVector([-0.1104, -0.0561, -0.0283])), Row(word='limitations', vector=DenseVector([-0.0153, 0.0514, 0.0426])), Row(word='same', vector=DenseVector([0.0804, -0.0343, -0.1475])), Row(word='analysis', vector=DenseVector([0.098, 0.1479, 0.1389])), Row(word='shown', vector=DenseVector([-0.096, -0.0804, -0.0132])), Row(word='since,', vector=DenseVector([0.0048, 0.1533, 0.0674])), Row(word='laws,', vector=DenseVector([-0.1586, 0.0382, 0.0121])), Row(word='focus', vector=DenseVector([-0.1038, -0.1129, -0.1522])), Row(word='differently,', vector=DenseVector([0.0778, -0.1518, -0.0307])), Row(word='apply', vector=DenseVector([-0.0838, -0.0411, 0.1012])), Row(word='individual', vector=DenseVector([-0.1752, -0.1254, -0.0822])), Row(word='interest', vector=DenseVector([-0.1406, 0.1589, 0.0825])), Row(word='geometrical', vector=DenseVector([0.0671, -0.0986, 0.032])), Row(word='particle', vector=DenseVector([0.1449, 0.1109, 0.0705])), Row(word='inhabitants', vector=DenseVector([-0.1131, 0.098, 0.053])), Row(word='carriers', vector=DenseVector([-0.1144, -0.1338, 0.1107])), Row(word='electron–phonon', vector=DenseVector([-0.1312, -0.148, -0.0788])), Row(word='role.', vector=DenseVector([-0.0364, 0.0847, -0.1471])), Row(word='itself', vector=DenseVector([0.0811, -0.0043, -0.0905])), Row(word='tenable.', vector=DenseVector([0.1141, 0.0371, -0.1215])), Row(word='mechanics,', vector=DenseVector([0.0969, -0.0221, 0.055])), Row(word='ray', vector=DenseVector([0.0572, 0.1093, -0.0488])), Row(word='them', vector=DenseVector([-0.1135, 0.0885, 0.1297])), Row(word='focusing', vector=DenseVector([-0.1093, 0.025, -0.0164])), Row(word='simple', vector=DenseVector([0.0588, 0.0977, -0.1023])), Row(word='beam', vector=DenseVector([-0.0085, -0.0161, -0.1541])), Row(word='by', vector=DenseVector([0.0235, 0.0375, -0.1428])), Row(word='libet', vector=DenseVector([0.1639, -0.1181, -0.0301])), Row(word='then', vector=DenseVector([-0.0177, -0.0014, -0.124])), Row(word='equation', vector=DenseVector([0.0689, 0.0151, -0.0851])), Row(word='coulomb', vector=DenseVector([-0.0921, 0.1057, 0.04])), Row(word='function', vector=DenseVector([-0.151, 0.1369, 0.0748])), Row(word='should', vector=DenseVector([-0.032, -0.0932, -0.1241])), Row(word='will', vector=DenseVector([-0.188, 0.0989, 0.0093])), Row(word='paper', vector=DenseVector([-0.0262, 0.0951, 0.1313])), Row(word='much', vector=DenseVector([-0.1676, -0.013, -0.1186])), Row(word='their', vector=DenseVector([0.0467, 0.1603, -0.0923])), Row(word='specific', vector=DenseVector([0.1167, 0.0489, 0.0111])), Row(word='not', vector=DenseVector([-0.0628, 0.0774, 0.0606])), Row(word='all-pervading', vector=DenseVector([-0.1515, -0.1283, 0.106])), Row(word='with', vector=DenseVector([0.001, -0.0103, 0.1478])), Row(word='from', vector=DenseVector([-0.0647, -0.0295, -0.1333])), Row(word=\"carriers'\", vector=DenseVector([0.0878, -0.1095, -0.1357])), Row(word='cell,', vector=DenseVector([0.1241, -0.1301, 0.0422])), Row(word='means', vector=DenseVector([0.0194, 0.1325, -0.1215])), Row(word='processes.', vector=DenseVector([-0.1257, 0.1536, -0.0439])), Row(word='reality', vector=DenseVector([0.1384, 0.1037, -0.076])), Row(word='controlled', vector=DenseVector([0.0269, -0.0805, -0.0884])), Row(word='empirically', vector=DenseVector([0.1598, 0.0047, 0.0125])), Row(word='dependence', vector=DenseVector([0.0024, 0.0694, 0.0983])), Row(word='theories)', vector=DenseVector([0.0117, -0.0136, -0.0163])), Row(word='quoted', vector=DenseVector([-0.0252, -0.0503, 0.126])), Row(word='example,', vector=DenseVector([0.0079, 0.1381, -0.0698])), Row(word='true', vector=DenseVector([-0.0825, -0.0131, -0.1075])), Row(word='al.', vector=DenseVector([0.0305, -0.1363, -0.1541])), Row(word='knowledge', vector=DenseVector([-0.1135, 0.0147, 0.018])), Row(word='theory', vector=DenseVector([0.003, -0.064, 0.1656])), Row(word='contained', vector=DenseVector([-0.1346, -0.0014, 0.0892])), Row(word='could', vector=DenseVector([0.0607, 0.0303, 0.0315])), Row(word='use', vector=DenseVector([0.0957, -0.0017, -0.0679])), Row(word='phenomena', vector=DenseVector([-0.0551, -0.0526, -0.1284])), Row(word='make', vector=DenseVector([-0.0576, -0.1621, 0.1035])), Row(word='put', vector=DenseVector([0.0757, -0.094, 0.0518])), Row(word='charge', vector=DenseVector([-0.063, -0.051, -0.1604])), Row(word='approach', vector=DenseVector([-0.01, -0.0073, -0.0769])), Row(word='(“eikonal”)', vector=DenseVector([-0.1312, -0.1587, 0.0219])), Row(word='paper,', vector=DenseVector([0.0477, 0.066, 0.0412])), Row(word='ghirardi–rimini–weber', vector=DenseVector([-0.0397, -0.0564, -0.0747])), Row(word='will.', vector=DenseVector([-0.1558, 0.0543, 0.0292])), Row(word='its', vector=DenseVector([-0.048, -0.1355, 0.0774])), Row(word='which', vector=DenseVector([-0.1755, -0.1337, -0.1035])), Row(word='onset', vector=DenseVector([0.0141, 0.0192, 0.0912])), Row(word='an', vector=DenseVector([-0.1514, 0.1099, 0.0896])), Row(word='cell', vector=DenseVector([-0.0231, 0.1557, 0.0494])), Row(word='spontaneous', vector=DenseVector([-0.1556, -0.0606, 0.1566])), Row(word='modalities.', vector=DenseVector([0.1088, 0.0201, -0.1604])), Row(word='interval;', vector=DenseVector([-0.1547, -0.1037, -0.0984])), Row(word='ruled', vector=DenseVector([0.1137, 0.0324, -0.0362])), Row(word='be', vector=DenseVector([0.1202, -0.065, 0.1419])), Row(word='own,', vector=DenseVector([0.0657, -0.1002, 0.0661])), Row(word='universe', vector=DenseVector([0.1679, -0.1623, -0.1473])), Row(word='turns', vector=DenseVector([0.0582, -0.0461, -0.1125])), Row(word='eikonal', vector=DenseVector([-0.0713, -0.0734, -0.0276])), Row(word='effort,', vector=DenseVector([0.0022, 0.123, -0.1621])), Row(word='ceylan', vector=DenseVector([0.0946, -0.0646, -0.0121])), Row(word='deterministic', vector=DenseVector([0.0046, 0.0244, -0.1527])), Row(word='dissociation', vector=DenseVector([-0.1815, -0.051, 0.1046])), Row(word='existence', vector=DenseVector([-0.0031, 0.1062, -0.1256])), Row(word='possess', vector=DenseVector([0.0721, 0.1687, -0.0087])), Row(word='time', vector=DenseVector([-0.0948, 0.0167, 0.1236])), Row(word='undergoes', vector=DenseVector([-0.1148, 0.0443, 0.1594])), Row(word='occur.', vector=DenseVector([0.0211, 0.0759, -0.0508])), Row(word='several', vector=DenseVector([0.1631, -0.035, -0.0355])), Row(word='approximation,', vector=DenseVector([-0.1545, -0.1211, -0.0454])), Row(word='more', vector=DenseVector([-0.0431, -0.0367, 0.0979])), Row(word='unceasing', vector=DenseVector([-0.1631, -0.1273, -0.0473])), Row(word='omniscient', vector=DenseVector([-0.1028, 0.1624, -0.1034])), Row(word='attempt', vector=DenseVector([-0.0014, 0.0828, 0.1018])), Row(word='type', vector=DenseVector([-0.0101, -0.053, -0.0448])), Row(word='mechanism', vector=DenseVector([-0.0655, -0.0141, -0.1366])), Row(word='wide', vector=DenseVector([0.0499, -0.0345, -0.1072])), Row(word='meaningful,', vector=DenseVector([-0.0644, -0.125, 0.0321])), Row(word='neuroscientific', vector=DenseVector([-0.141, -0.0013, -0.1575])), Row(word='(grw)', vector=DenseVector([-0.0958, -0.161, 0.1373])), Row(word='orthodox', vector=DenseVector([-0.0105, 0.104, 0.0273])), Row(word='living.', vector=DenseVector([-0.0473, -0.049, -0.0613])), Row(word='terms', vector=DenseVector([-0.1267, -0.0785, 0.0841])), Row(word='novel', vector=DenseVector([0.0422, 0.1046, 0.0425])), Row(word='uninhibited', vector=DenseVector([-0.1574, -0.05, 0.0624])), Row(word='system.', vector=DenseVector([-0.042, 0.0096, 0.1625])), Row(word='collapses', vector=DenseVector([0.0581, 0.0451, -0.1437])), Row(word='impetus', vector=DenseVector([0.1505, 0.0623, 0.0873])), Row(word='about', vector=DenseVector([-0.1403, -0.1345, -0.1257])), Row(word='simply', vector=DenseVector([-0.1004, 0.1026, -0.0281])), Row(word='family', vector=DenseVector([0.1495, 0.0625, 0.0308])), Row(word='literature.', vector=DenseVector([-0.1125, 0.1347, -0.0765])), Row(word='geminate', vector=DenseVector([-0.0254, 0.1025, 0.1032])), Row(word='schrödinger', vector=DenseVector([0.0704, 0.0826, 0.0929])), Row(word='mathematically', vector=DenseVector([-0.0283, -0.052, -0.0528])), Row(word='types', vector=DenseVector([-0.0774, 0.0133, 0.105])), Row(word='light', vector=DenseVector([-0.1015, 0.0874, 0.0636])), Row(word='describing', vector=DenseVector([-0.0863, 0.0913, -0.0935])), Row(word='call', vector=DenseVector([-0.0966, 0.1331, -0.0876])), Row(word='essence', vector=DenseVector([-0.0446, 0.0203, -0.0121])), Row(word='word', vector=DenseVector([-0.0489, -0.1364, -0.0803])), Row(word='efficiency.', vector=DenseVector([-0.1674, -0.1471, -0.1551])), Row(word='inhibition', vector=DenseVector([-0.0662, 0.1351, -0.097])), Row(word='during', vector=DenseVector([0.0059, 0.1056, 0.056])), Row(word='(2017)', vector=DenseVector([0.1633, -0.0621, 0.1637])), Row(word='accurately', vector=DenseVector([-0.1299, 0.0568, -0.0525])), Row(word='very', vector=DenseVector([0.0452, 0.0437, -0.0464])), Row(word='we', vector=DenseVector([-0.0764, 0.0675, 0.1374])), Row(word='equation,', vector=DenseVector([-0.0412, -0.1377, -0.1558])), Row(word='analog', vector=DenseVector([-0.0779, 0.0952, 0.0108])), Row(word='parameters', vector=DenseVector([-0.068, 0.0493, -0.1619])), Row(word='limits', vector=DenseVector([0.1215, 0.1444, -0.1194])), Row(word='performance', vector=DenseVector([0.0943, -0.0283, -0.0919])), Row(word='pair,', vector=DenseVector([-0.0923, 0.1716, 0.1318])), Row(word='autonomy', vector=DenseVector([0.0492, -0.1493, 0.1195])), Row(word='known', vector=DenseVector([0.0632, 0.0467, -0.1425])), Row(word='mental', vector=DenseVector([0.1286, 0.0001, 0.0221])), Row(word='action', vector=DenseVector([-0.0385, 0.127, 0.1192])), Row(word='fact,', vector=DenseVector([-0.0642, 0.0676, 0.0707])), Row(word='study', vector=DenseVector([-0.1102, -0.1107, 0.1408])), Row(word='compare', vector=DenseVector([0.0113, 0.0292, 0.0105])), Row(word='like', vector=DenseVector([0.02, 0.0907, -0.029])), Row(word='other', vector=DenseVector([-0.1051, -0.0556, -0.1351])), Row(word='of', vector=DenseVector([-0.116, 0.0714, -0.0658])), Row(word='states', vector=DenseVector([-0.0352, -0.1017, 0.1529])), Row(word='containing', vector=DenseVector([0.0293, -0.0299, 0.1167])), Row(word='and', vector=DenseVector([-0.2291, 0.1073, 0.0488])), Row(word='primary', vector=DenseVector([-0.0953, -0.1253, 0.163])), Row(word='one', vector=DenseVector([0.0945, -0.1683, -0.1001])), Row(word='free', vector=DenseVector([0.0391, -0.0878, 0.1411])), Row(word='classical', vector=DenseVector([-0.1068, -0.0918, -0.0272])), Row(word='beyond', vector=DenseVector([0.0101, -0.0826, -0.0076])), Row(word='quantum', vector=DenseVector([-0.1037, 0.1073, -0.057])), Row(word='holding', vector=DenseVector([-0.1675, 0.1564, 0.0863])), Row(word='refractive', vector=DenseVector([0.0946, -0.0177, -0.1027])), Row(word='investigate', vector=DenseVector([-0.0113, 0.0133, -0.0212])), Row(word='view', vector=DenseVector([0.1611, 0.0806, 0.0597])), Row(word='level', vector=DenseVector([-0.1373, 0.0276, -0.009])), Row(word='investigations.', vector=DenseVector([0.0241, 0.1458, 0.1208])), Row(word='the', vector=DenseVector([-0.4258, 0.1632, 0.086])), Row(word='physics,', vector=DenseVector([-0.0344, -0.0812, -0.1427])), Row(word='helmholtz-like', vector=DenseVector([-0.0223, 0.1452, -0.1036])), Row(word='physical', vector=DenseVector([0.0812, 0.0667, 0.0974])), Row(word='those', vector=DenseVector([0.0967, 0.0594, 0.0127])), Row(word='only', vector=DenseVector([0.037, -0.0651, 0.1358]))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1YQmNdCZPY5",
        "outputId": "bf75d508-b7cb-4304-fdd3-25918f1a500c"
      },
      "source": [
        "# Show the synonyms and cosine similarity of the word in input data\r\n",
        "synonyms = model.findSynonyms(\"science\", 10)\r\n",
        "synonyms.show(10)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+------------------+\n",
            "|       word|        similarity|\n",
            "+-----------+------------------+\n",
            "| describing|0.9997835755348206|\n",
            "|      prior|0.9970971345901489|\n",
            "|       call| 0.979093074798584|\n",
            "|      don’t|0.9779413342475891|\n",
            "|       term|0.9775862097740173|\n",
            "|investigate| 0.971872091293335|\n",
            "| omniscient|0.9716155529022217|\n",
            "|literature.|0.9690399765968323|\n",
            "|      case.|0.9689924120903015|\n",
            "|     field,|0.9689012765884399|\n",
            "+-----------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzQ7KVW3Riz0"
      },
      "source": [
        "# creating spark session\r\n",
        "spark4 = SparkSession.builder.appName(\"Word2Vec Similarity\").getOrCreate()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzHNuEa5Ri9E"
      },
      "source": [
        "# Create the dataframe with five text abstracts\r\n",
        "abst = spark4.read.text('input*.txt')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YCvKRLidGpP"
      },
      "source": [
        "# Tokenize the abstract texts\r\n",
        "tokenizer = Tokenizer(inputCol=\"value\", outputCol=\"words\")\r\n",
        "wordsData = tokenizer.transform(abst)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IjCO6rRRjCu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98a9dec4-44e2-4b4b-d00b-71310fe06384"
      },
      "source": [
        "# Create a mapping from words to vectors\r\n",
        "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"words\", outputCol=\"result\")\r\n",
        "model = word2Vec.fit(wordsData)\r\n",
        "print(model.getVectors().collect())\r\n",
        "result = model.getVectors().collect()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Row(word='beneath', vector=DenseVector([-0.063, -0.1433, -0.0275])), Row(word='used', vector=DenseVector([0.1425, 0.0271, 0.1235])), Row(word='providing', vector=DenseVector([0.0157, 0.098, -0.1096])), Row(word='cells', vector=DenseVector([-0.0462, -0.0827, -0.1413])), Row(word='measure', vector=DenseVector([0.0346, 0.0103, 0.1353])), Row(word='is,', vector=DenseVector([-0.0772, -0.1257, 0.1589])), Row(word='number', vector=DenseVector([-0.1333, -0.0183, 0.1237])), Row(word='for', vector=DenseVector([-0.0761, 0.1601, -0.0055])), Row(word='hamiltonian', vector=DenseVector([0.1538, 0.0424, -0.0704])), Row(word='find', vector=DenseVector([0.1352, -0.1649, 0.1625])), Row(word='factual', vector=DenseVector([0.1439, 0.1665, 0.1454])), Row(word='superconscious', vector=DenseVector([-0.0677, -0.011, 0.0132])), Row(word='shifts', vector=DenseVector([-0.0902, -0.1046, -0.1303])), Row(word='due', vector=DenseVector([0.1268, 0.0562, 0.0994])), Row(word='proposed', vector=DenseVector([-0.0757, -0.0869, -0.1081])), Row(word='any', vector=DenseVector([0.0894, 0.001, 0.0699])), Row(word='lead', vector=DenseVector([-0.1504, 0.1371, 0.0823])), Row(word='undecidable.', vector=DenseVector([0.1591, 0.0633, 0.034])), Row(word='experiments', vector=DenseVector([0.1016, 0.1439, -0.0483])), Row(word='approach;', vector=DenseVector([-0.0807, -0.0211, 0.0165])), Row(word='operations', vector=DenseVector([0.0689, -0.0238, -0.0536])), Row(word='this', vector=DenseVector([0.0381, -0.1084, 0.0145])), Row(word='in', vector=DenseVector([0.1401, -0.1509, -0.0453])), Row(word='recombination', vector=DenseVector([0.0159, 0.0158, 0.1202])), Row(word='inspired', vector=DenseVector([-0.1583, 0.1131, 0.0766])), Row(word='provided', vector=DenseVector([0.0632, -0.164, -0.0283])), Row(word='attention,', vector=DenseVector([0.1613, -0.1311, -0.1568])), Row(word='have', vector=DenseVector([0.0877, -0.0735, 0.1397])), Row(word='point', vector=DenseVector([-0.0249, 0.0218, -0.0449])), Row(word='are', vector=DenseVector([-0.1208, -0.0561, 0.0796])), Row(word='is', vector=DenseVector([0.0292, -0.0396, 0.1132])), Row(word='absence', vector=DenseVector([-0.0978, -0.0825, -0.0711])), Row(word='claim', vector=DenseVector([0.0149, 0.157, 0.0927])), Row(word='among', vector=DenseVector([0.0465, 0.0817, 0.1324])), Row(word='governs', vector=DenseVector([-0.0087, -0.0889, -0.0801])), Row(word='force', vector=DenseVector([0.0349, -0.0802, 0.0697])), Row(word='show', vector=DenseVector([0.0741, -0.1623, 0.1442])), Row(word='grw', vector=DenseVector([0.1549, -0.1376, 0.029])), Row(word='independent', vector=DenseVector([0.1453, 0.0152, -0.0456])), Row(word='but,', vector=DenseVector([-0.0381, -0.1197, 0.0518])), Row(word='given', vector=DenseVector([0.1065, 0.1558, -0.0968])), Row(word='hence', vector=DenseVector([0.1554, -0.1071, 0.0683])), Row(word='system', vector=DenseVector([0.1723, -0.015, -0.1151])), Row(word='scientific', vector=DenseVector([0.0007, -0.0285, 0.1615])), Row(word='prior', vector=DenseVector([-0.1208, 0.1237, -0.1534])), Row(word='examined', vector=DenseVector([-0.1408, -0.0718, 0.1125])), Row(word='interaction', vector=DenseVector([-0.0506, -0.008, -0.0731])), Row(word='model', vector=DenseVector([-0.0822, -0.0863, -0.1285])), Row(word='empirical', vector=DenseVector([0.1556, -0.1035, 0.1311])), Row(word='observer-induced', vector=DenseVector([-0.0024, -0.0321, -0.1496])), Row(word='but', vector=DenseVector([-0.1282, 0.1486, -0.0047])), Row(word='analyse', vector=DenseVector([-0.1486, 0.1636, -0.0636])), Row(word='whether', vector=DenseVector([-0.0425, -0.0214, -0.0652])), Row(word='optimize', vector=DenseVector([-0.1651, 0.0628, 0.0488])), Row(word='physics', vector=DenseVector([-0.1613, 0.084, -0.0335])), Row(word='quantify', vector=DenseVector([-0.0957, 0.0409, -0.1418])), Row(word='potentials', vector=DenseVector([0.0204, -0.0481, 0.0783])), Row(word='best', vector=DenseVector([-0.0984, -0.1505, 0.0998])), Row(word='subjective', vector=DenseVector([-0.0164, 0.1176, 0.1191])), Row(word='known.', vector=DenseVector([-0.0652, 0.0367, 0.0369])), Row(word='collapse,', vector=DenseVector([-0.1485, -0.112, -0.1492])), Row(word='essential', vector=DenseVector([-0.0724, -0.1233, -0.0902])), Row(word='similar', vector=DenseVector([0.1362, 0.0019, 0.0223])), Row(word='trajectories,', vector=DenseVector([0.1459, -0.0201, 0.0077])), Row(word='what', vector=DenseVector([-0.0051, 0.1525, -0.1614])), Row(word='(and', vector=DenseVector([0.0979, 0.038, -0.0964])), Row(word='recent', vector=DenseVector([0.0095, -0.0438, 0.076])), Row(word='hypothesis', vector=DenseVector([-0.1676, 0.0467, 0.121])), Row(word='problem', vector=DenseVector([0.1003, 0.0472, -0.0611])), Row(word='versions', vector=DenseVector([-0.1129, 0.1624, -0.0347])), Row(word='theory.', vector=DenseVector([-0.0157, -0.0651, -0.0112])), Row(word='up', vector=DenseVector([0.0919, 0.077, -0.1017])), Row(word='going', vector=DenseVector([0.0082, -0.1061, 0.1008])), Row(word='uses', vector=DenseVector([0.1294, -0.0754, -0.1051])), Row(word='our', vector=DenseVector([-0.1131, 0.1066, 0.1145])), Row(word='zero', vector=DenseVector([0.068, -0.0965, 0.0174])), Row(word='considers', vector=DenseVector([0.0592, -0.1116, -0.1301])), Row(word='play', vector=DenseVector([-0.0919, -0.1508, 0.0])), Row(word='purpose,', vector=DenseVector([0.0954, 0.0607, 0.1222])), Row(word='electron–hole', vector=DenseVector([-0.1294, 0.1456, 0.0387])), Row(word='all', vector=DenseVector([-0.0484, -0.0223, -0.0632])), Row(word='studies', vector=DenseVector([0.0452, -0.0635, 0.0277])), Row(word='theory).', vector=DenseVector([0.07, 0.1051, -0.0922])), Row(word='present', vector=DenseVector([0.0843, 0.0406, -0.1225])), Row(word='facts', vector=DenseVector([-0.1033, 0.033, 0.0358])), Row(word='integrating', vector=DenseVector([0.0451, 0.0838, 0.0343])), Row(word='consciousness', vector=DenseVector([-0.1379, 0.1194, 0.076])), Row(word='idea', vector=DenseVector([-0.1351, 0.1122, -0.0277])), Row(word='equations', vector=DenseVector([0.1655, 0.0413, 0.0979])), Row(word='medium,', vector=DenseVector([0.1364, 0.1519, -0.0108])), Row(word='examine', vector=DenseVector([0.1034, -0.042, 0.1071])), Row(word='one’s', vector=DenseVector([-0.0092, -0.1152, -0.1395])), Row(word='possibilities', vector=DenseVector([-0.094, -0.1277, 0.1646])), Row(word='us', vector=DenseVector([0.0584, 0.1205, -0.0384])), Row(word='understand', vector=DenseVector([0.0186, 0.1265, -0.0556])), Row(word='it', vector=DenseVector([-0.1437, 0.0743, -0.0463])), Row(word='case.', vector=DenseVector([-0.1033, 0.0861, -0.166])), Row(word='reintroduced', vector=DenseVector([-0.0783, 0.1076, -0.0108])), Row(word='impact', vector=DenseVector([-0.1308, -0.141, 0.1115])), Row(word='soul', vector=DenseVector([-0.0991, 0.003, -0.0161])), Row(word='separation', vector=DenseVector([0.0369, 0.0492, -0.007])), Row(word='literature', vector=DenseVector([-0.1382, 0.049, -0.0637])), Row(word='self', vector=DenseVector([-0.1072, -0.164, 0.0805])), Row(word='laws', vector=DenseVector([0.0295, -0.1273, 0.1156])), Row(word='term', vector=DenseVector([-0.1625, 0.1502, -0.1045])), Row(word='a', vector=DenseVector([-0.1126, -0.1504, -0.112])), Row(word='tight-binding', vector=DenseVector([-0.1216, -0.165, -0.0008])), Row(word='moving', vector=DenseVector([0.1617, 0.0573, 0.008])), Row(word='study,', vector=DenseVector([-0.0007, -0.0326, 0.1583])), Row(word='wave-like', vector=DenseVector([-0.1672, 0.0811, 0.0675])), Row(word='don’t', vector=DenseVector([-0.0837, 0.0842, -0.0558])), Row(word='receive', vector=DenseVector([0.0288, -0.1249, -0.1204])), Row(word='attention', vector=DenseVector([0.1481, -0.0166, -0.0295])), Row(word='field,', vector=DenseVector([-0.0548, 0.0885, -0.0551])), Row(word='within', vector=DenseVector([-0.1283, 0.1032, 0.0145])), Row(word='as', vector=DenseVector([0.1441, 0.0661, 0.0266])), Row(word='yield', vector=DenseVector([0.0884, 0.1056, -0.1474])), Row(word='knowledge,', vector=DenseVector([0.0951, -0.0455, -0.0339])), Row(word='has', vector=DenseVector([-0.085, -0.0409, 0.0694])), Row(word='opposed', vector=DenseVector([0.07, -0.0857, -0.0557])), Row(word='demonstrate', vector=DenseVector([0.0164, 0.0097, 0.1619])), Row(word='theories', vector=DenseVector([-0.1497, 0.0745, 0.0622])), Row(word='moreover,', vector=DenseVector([0.0523, -0.1041, 0.1516])), Row(word='trajectories', vector=DenseVector([-0.0791, -0.0489, 0.009])), Row(word='bohmian', vector=DenseVector([-0.0416, -0.1232, 0.016])), Row(word='certain', vector=DenseVector([0.1368, -0.1005, -0.0092])), Row(word='mechanics', vector=DenseVector([-0.0909, -0.0208, 0.136])), Row(word='footing', vector=DenseVector([0.1023, 0.0187, 0.0861])), Row(word='optics', vector=DenseVector([0.087, -0.1525, -0.1648])), Row(word='concepts', vector=DenseVector([-0.0431, 0.1673, 0.1063])), Row(word='numerical', vector=DenseVector([0.109, -0.0642, -0.0913])), Row(word='or', vector=DenseVector([0.0114, 0.0634, 0.0905])), Row(word='describe', vector=DenseVector([0.1166, -0.131, -0.0902])), Row(word='mechanics.', vector=DenseVector([-0.0349, 0.0511, -0.0893])), Row(word='coupling,', vector=DenseVector([-0.0341, -0.0265, -0.0282])), Row(word='processing', vector=DenseVector([-0.0142, -0.0018, 0.0305])), Row(word='questions', vector=DenseVector([-0.0053, -0.0637, -0.0932])), Row(word='come', vector=DenseVector([-0.1758, 0.1491, 0.1211])), Row(word='such', vector=DenseVector([-0.0321, -0.0646, 0.0265])), Row(word='they', vector=DenseVector([-0.0381, -0.0613, 0.131])), Row(word='context,', vector=DenseVector([-0.0005, 0.0038, -0.0816])), Row(word='activity.', vector=DenseVector([0.0549, -0.0118, 0.0414])), Row(word='et', vector=DenseVector([0.0436, 0.0879, -0.0078])), Row(word='limiting', vector=DenseVector([0.0806, 0.1472, 0.1377])), Row(word='decision', vector=DenseVector([-0.0841, 0.0035, -0.0492])), Row(word='mathematics', vector=DenseVector([-0.0071, -0.0742, 0.0714])), Row(word='optical', vector=DenseVector([0.0193, -0.1026, -0.1307])), Row(word='(including', vector=DenseVector([0.0771, 0.1093, -0.0207])), Row(word='serves', vector=DenseVector([0.0299, -0.0737, 0.1538])), Row(word='inexpensive', vector=DenseVector([0.1082, 0.1081, -0.1089])), Row(word='collective', vector=DenseVector([-0.1355, 0.0153, 0.1578])), Row(word='conscious', vector=DenseVector([-0.107, 0.0504, -0.0508])), Row(word='solar', vector=DenseVector([0.0095, 0.1189, -0.0511])), Row(word='subtly', vector=DenseVector([0.0299, -0.0339, 0.1162])), Row(word='it,', vector=DenseVector([0.1554, -0.0145, 0.1659])), Row(word='synonym', vector=DenseVector([-0.0488, -0.1408, -0.0964])), Row(word='(as', vector=DenseVector([-0.0123, 0.0917, 0.1372])), Row(word='that', vector=DenseVector([0.0014, -0.1072, 0.1209])), Row(word='out', vector=DenseVector([0.1495, -0.1198, 0.1084])), Row(word='cannot', vector=DenseVector([-0.0214, -0.098, 0.0127])), Row(word='(2017).', vector=DenseVector([0.0673, -0.1077, -0.0588])), Row(word='diffraction', vector=DenseVector([-0.1147, 0.0649, 0.1074])), Row(word='possibility', vector=DenseVector([-0.0353, 0.1422, -0.1081])), Row(word='to', vector=DenseVector([-0.2162, -0.0654, 0.0113])), Row(word='science', vector=DenseVector([-0.0855, 0.0865, -0.0924])), Row(word='direct', vector=DenseVector([-0.027, -0.1645, 0.0177])), Row(word='exact', vector=DenseVector([-0.0609, 0.145, -0.0603])), Row(word='expand', vector=DenseVector([-0.0853, -0.1053, 0.0741])), Row(word='helmholtz', vector=DenseVector([0.0583, 0.0339, -0.1309])), Row(word='dynamical', vector=DenseVector([-0.0942, 0.1368, 0.1364])), Row(word='associated', vector=DenseVector([-0.1156, 0.0113, -0.1051])), Row(word='interference)', vector=DenseVector([-0.094, 0.0688, 0.0007])), Row(word='traditional', vector=DenseVector([0.1156, 0.0889, -0.1072])), Row(word='findings', vector=DenseVector([-0.0097, -0.0067, 0.1007])), Row(word='fact', vector=DenseVector([0.0107, 0.0254, -0.0428])), Row(word='organic', vector=DenseVector([0.1672, 0.1412, -0.0377])), Row(word='carriers.', vector=DenseVector([-0.1265, -0.1093, 0.1384])), Row(word='awareness', vector=DenseVector([-0.0303, -0.1291, 0.0837])), Row(word='most', vector=DenseVector([0.1448, 0.0278, -0.1646])), Row(word='here', vector=DenseVector([0.0158, -0.156, 0.1465])), Row(word='neuroscience', vector=DenseVector([0.135, -0.0921, 0.1461])), Row(word='(no-collapse)', vector=DenseVector([0.0073, 0.1175, 0.1163])), Row(word='help', vector=DenseVector([-0.0091, 0.1435, 0.0235])), Row(word='discussed', vector=DenseVector([0.0791, 0.1173, 0.0563])), Row(word='there', vector=DenseVector([0.1167, -0.0809, -0.0803])), Row(word='reliably', vector=DenseVector([0.0801, -0.0693, 0.0036])), Row(word='stationary', vector=DenseVector([-0.0935, -0.1111, 0.0376])), Row(word='discuss', vector=DenseVector([-0.065, 0.0626, -0.0141])), Row(word='meditation', vector=DenseVector([0.0892, 0.1143, 0.1154])), Row(word='through', vector=DenseVector([0.0677, 0.0851, -0.1395])), Row(word='points', vector=DenseVector([-0.1749, -0.0689, 0.09])), Row(word='readiness', vector=DenseVector([-0.0141, 0.1588, 0.1519])), Row(word='mechanisms.', vector=DenseVector([0.145, 0.1421, 0.1132])), Row(word='leads', vector=DenseVector([0.0509, 0.1065, -0.1309])), Row(word='universe.', vector=DenseVector([-0.0789, -0.0451, -0.1055])), Row(word='results', vector=DenseVector([-0.1495, 0.1244, -0.0144])), Row(word='wave', vector=DenseVector([-0.1481, -0.0413, 0.1665])), Row(word='limit.', vector=DenseVector([0.1466, -0.1346, 0.1383])), Row(word='subconscious', vector=DenseVector([0.0172, 0.0271, -0.1555])), Row(word='also', vector=DenseVector([0.1659, -0.165, -0.0482])), Row(word='processes', vector=DenseVector([-0.075, -0.0939, 0.0352])), Row(word='on', vector=DenseVector([-0.0022, -0.0945, 0.0283])), Row(word='method,', vector=DenseVector([-0.1104, -0.0561, -0.0283])), Row(word='limitations', vector=DenseVector([-0.0153, 0.0514, 0.0426])), Row(word='same', vector=DenseVector([0.0804, -0.0343, -0.1475])), Row(word='analysis', vector=DenseVector([0.098, 0.1479, 0.1389])), Row(word='shown', vector=DenseVector([-0.096, -0.0804, -0.0132])), Row(word='since,', vector=DenseVector([0.0048, 0.1533, 0.0674])), Row(word='laws,', vector=DenseVector([-0.1586, 0.0382, 0.0121])), Row(word='focus', vector=DenseVector([-0.1038, -0.1129, -0.1522])), Row(word='differently,', vector=DenseVector([0.0778, -0.1518, -0.0307])), Row(word='apply', vector=DenseVector([-0.0838, -0.0411, 0.1012])), Row(word='individual', vector=DenseVector([-0.1752, -0.1254, -0.0822])), Row(word='interest', vector=DenseVector([-0.1406, 0.1589, 0.0825])), Row(word='geometrical', vector=DenseVector([0.0671, -0.0986, 0.032])), Row(word='particle', vector=DenseVector([0.1449, 0.1109, 0.0705])), Row(word='inhabitants', vector=DenseVector([-0.1131, 0.098, 0.053])), Row(word='carriers', vector=DenseVector([-0.1144, -0.1338, 0.1107])), Row(word='electron–phonon', vector=DenseVector([-0.1312, -0.148, -0.0788])), Row(word='role.', vector=DenseVector([-0.0364, 0.0847, -0.1471])), Row(word='itself', vector=DenseVector([0.0811, -0.0043, -0.0905])), Row(word='tenable.', vector=DenseVector([0.1141, 0.0371, -0.1215])), Row(word='mechanics,', vector=DenseVector([0.0969, -0.0221, 0.055])), Row(word='ray', vector=DenseVector([0.0572, 0.1093, -0.0488])), Row(word='them', vector=DenseVector([-0.1135, 0.0885, 0.1297])), Row(word='focusing', vector=DenseVector([-0.1093, 0.025, -0.0164])), Row(word='simple', vector=DenseVector([0.0588, 0.0977, -0.1023])), Row(word='beam', vector=DenseVector([-0.0085, -0.0161, -0.1541])), Row(word='by', vector=DenseVector([0.0235, 0.0375, -0.1428])), Row(word='libet', vector=DenseVector([0.1639, -0.1181, -0.0301])), Row(word='then', vector=DenseVector([-0.0177, -0.0014, -0.124])), Row(word='equation', vector=DenseVector([0.0689, 0.0151, -0.0851])), Row(word='coulomb', vector=DenseVector([-0.0921, 0.1057, 0.04])), Row(word='function', vector=DenseVector([-0.151, 0.1369, 0.0748])), Row(word='should', vector=DenseVector([-0.032, -0.0932, -0.1241])), Row(word='will', vector=DenseVector([-0.188, 0.0989, 0.0093])), Row(word='paper', vector=DenseVector([-0.0262, 0.0951, 0.1313])), Row(word='much', vector=DenseVector([-0.1676, -0.013, -0.1186])), Row(word='their', vector=DenseVector([0.0467, 0.1603, -0.0923])), Row(word='specific', vector=DenseVector([0.1167, 0.0489, 0.0111])), Row(word='not', vector=DenseVector([-0.0628, 0.0774, 0.0606])), Row(word='all-pervading', vector=DenseVector([-0.1515, -0.1283, 0.106])), Row(word='with', vector=DenseVector([0.001, -0.0103, 0.1478])), Row(word='from', vector=DenseVector([-0.0647, -0.0295, -0.1333])), Row(word=\"carriers'\", vector=DenseVector([0.0878, -0.1095, -0.1357])), Row(word='cell,', vector=DenseVector([0.1241, -0.1301, 0.0422])), Row(word='means', vector=DenseVector([0.0194, 0.1325, -0.1215])), Row(word='processes.', vector=DenseVector([-0.1257, 0.1536, -0.0439])), Row(word='reality', vector=DenseVector([0.1384, 0.1037, -0.076])), Row(word='controlled', vector=DenseVector([0.0269, -0.0805, -0.0884])), Row(word='empirically', vector=DenseVector([0.1598, 0.0047, 0.0125])), Row(word='dependence', vector=DenseVector([0.0024, 0.0694, 0.0983])), Row(word='theories)', vector=DenseVector([0.0117, -0.0136, -0.0163])), Row(word='quoted', vector=DenseVector([-0.0252, -0.0503, 0.126])), Row(word='example,', vector=DenseVector([0.0079, 0.1381, -0.0698])), Row(word='true', vector=DenseVector([-0.0825, -0.0131, -0.1075])), Row(word='al.', vector=DenseVector([0.0305, -0.1363, -0.1541])), Row(word='knowledge', vector=DenseVector([-0.1135, 0.0147, 0.018])), Row(word='theory', vector=DenseVector([0.003, -0.064, 0.1656])), Row(word='contained', vector=DenseVector([-0.1346, -0.0014, 0.0892])), Row(word='could', vector=DenseVector([0.0607, 0.0303, 0.0315])), Row(word='use', vector=DenseVector([0.0957, -0.0017, -0.0679])), Row(word='phenomena', vector=DenseVector([-0.0551, -0.0526, -0.1284])), Row(word='make', vector=DenseVector([-0.0576, -0.1621, 0.1035])), Row(word='put', vector=DenseVector([0.0757, -0.094, 0.0518])), Row(word='charge', vector=DenseVector([-0.063, -0.051, -0.1604])), Row(word='approach', vector=DenseVector([-0.01, -0.0073, -0.0769])), Row(word='(“eikonal”)', vector=DenseVector([-0.1312, -0.1587, 0.0219])), Row(word='paper,', vector=DenseVector([0.0477, 0.066, 0.0412])), Row(word='ghirardi–rimini–weber', vector=DenseVector([-0.0397, -0.0564, -0.0747])), Row(word='will.', vector=DenseVector([-0.1558, 0.0543, 0.0292])), Row(word='its', vector=DenseVector([-0.048, -0.1355, 0.0774])), Row(word='which', vector=DenseVector([-0.1755, -0.1337, -0.1035])), Row(word='onset', vector=DenseVector([0.0141, 0.0192, 0.0912])), Row(word='an', vector=DenseVector([-0.1514, 0.1099, 0.0896])), Row(word='cell', vector=DenseVector([-0.0231, 0.1557, 0.0494])), Row(word='spontaneous', vector=DenseVector([-0.1556, -0.0606, 0.1566])), Row(word='modalities.', vector=DenseVector([0.1088, 0.0201, -0.1604])), Row(word='interval;', vector=DenseVector([-0.1547, -0.1037, -0.0984])), Row(word='ruled', vector=DenseVector([0.1137, 0.0324, -0.0362])), Row(word='be', vector=DenseVector([0.1202, -0.065, 0.1419])), Row(word='own,', vector=DenseVector([0.0657, -0.1002, 0.0661])), Row(word='universe', vector=DenseVector([0.1679, -0.1623, -0.1473])), Row(word='turns', vector=DenseVector([0.0582, -0.0461, -0.1125])), Row(word='eikonal', vector=DenseVector([-0.0713, -0.0734, -0.0276])), Row(word='effort,', vector=DenseVector([0.0022, 0.123, -0.1621])), Row(word='ceylan', vector=DenseVector([0.0946, -0.0646, -0.0121])), Row(word='deterministic', vector=DenseVector([0.0046, 0.0244, -0.1527])), Row(word='dissociation', vector=DenseVector([-0.1815, -0.051, 0.1046])), Row(word='existence', vector=DenseVector([-0.0031, 0.1062, -0.1256])), Row(word='possess', vector=DenseVector([0.0721, 0.1687, -0.0087])), Row(word='time', vector=DenseVector([-0.0948, 0.0167, 0.1236])), Row(word='undergoes', vector=DenseVector([-0.1148, 0.0443, 0.1594])), Row(word='occur.', vector=DenseVector([0.0211, 0.0759, -0.0508])), Row(word='several', vector=DenseVector([0.1631, -0.035, -0.0355])), Row(word='approximation,', vector=DenseVector([-0.1545, -0.1211, -0.0454])), Row(word='more', vector=DenseVector([-0.0431, -0.0367, 0.0979])), Row(word='unceasing', vector=DenseVector([-0.1631, -0.1273, -0.0473])), Row(word='omniscient', vector=DenseVector([-0.1028, 0.1624, -0.1034])), Row(word='attempt', vector=DenseVector([-0.0014, 0.0828, 0.1018])), Row(word='type', vector=DenseVector([-0.0101, -0.053, -0.0448])), Row(word='mechanism', vector=DenseVector([-0.0655, -0.0141, -0.1366])), Row(word='wide', vector=DenseVector([0.0499, -0.0345, -0.1072])), Row(word='meaningful,', vector=DenseVector([-0.0644, -0.125, 0.0321])), Row(word='neuroscientific', vector=DenseVector([-0.141, -0.0013, -0.1575])), Row(word='(grw)', vector=DenseVector([-0.0958, -0.161, 0.1373])), Row(word='orthodox', vector=DenseVector([-0.0105, 0.104, 0.0273])), Row(word='living.', vector=DenseVector([-0.0473, -0.049, -0.0613])), Row(word='terms', vector=DenseVector([-0.1267, -0.0785, 0.0841])), Row(word='novel', vector=DenseVector([0.0422, 0.1046, 0.0425])), Row(word='uninhibited', vector=DenseVector([-0.1574, -0.05, 0.0624])), Row(word='system.', vector=DenseVector([-0.042, 0.0096, 0.1625])), Row(word='collapses', vector=DenseVector([0.0581, 0.0451, -0.1437])), Row(word='impetus', vector=DenseVector([0.1505, 0.0623, 0.0873])), Row(word='about', vector=DenseVector([-0.1403, -0.1345, -0.1257])), Row(word='simply', vector=DenseVector([-0.1004, 0.1026, -0.0281])), Row(word='family', vector=DenseVector([0.1495, 0.0625, 0.0308])), Row(word='literature.', vector=DenseVector([-0.1125, 0.1347, -0.0765])), Row(word='geminate', vector=DenseVector([-0.0254, 0.1025, 0.1032])), Row(word='schrödinger', vector=DenseVector([0.0704, 0.0826, 0.0929])), Row(word='mathematically', vector=DenseVector([-0.0283, -0.052, -0.0528])), Row(word='types', vector=DenseVector([-0.0774, 0.0133, 0.105])), Row(word='light', vector=DenseVector([-0.1015, 0.0874, 0.0636])), Row(word='describing', vector=DenseVector([-0.0863, 0.0913, -0.0935])), Row(word='call', vector=DenseVector([-0.0966, 0.1331, -0.0876])), Row(word='essence', vector=DenseVector([-0.0446, 0.0203, -0.0121])), Row(word='word', vector=DenseVector([-0.0489, -0.1364, -0.0803])), Row(word='efficiency.', vector=DenseVector([-0.1674, -0.1471, -0.1551])), Row(word='inhibition', vector=DenseVector([-0.0662, 0.1351, -0.097])), Row(word='during', vector=DenseVector([0.0059, 0.1056, 0.056])), Row(word='(2017)', vector=DenseVector([0.1633, -0.0621, 0.1637])), Row(word='accurately', vector=DenseVector([-0.1299, 0.0568, -0.0525])), Row(word='very', vector=DenseVector([0.0452, 0.0437, -0.0464])), Row(word='we', vector=DenseVector([-0.0764, 0.0675, 0.1374])), Row(word='equation,', vector=DenseVector([-0.0412, -0.1377, -0.1558])), Row(word='analog', vector=DenseVector([-0.0779, 0.0952, 0.0108])), Row(word='parameters', vector=DenseVector([-0.068, 0.0493, -0.1619])), Row(word='limits', vector=DenseVector([0.1215, 0.1444, -0.1194])), Row(word='performance', vector=DenseVector([0.0943, -0.0283, -0.0919])), Row(word='pair,', vector=DenseVector([-0.0923, 0.1716, 0.1318])), Row(word='autonomy', vector=DenseVector([0.0492, -0.1493, 0.1195])), Row(word='known', vector=DenseVector([0.0632, 0.0467, -0.1425])), Row(word='mental', vector=DenseVector([0.1286, 0.0001, 0.0221])), Row(word='action', vector=DenseVector([-0.0385, 0.127, 0.1192])), Row(word='fact,', vector=DenseVector([-0.0642, 0.0676, 0.0707])), Row(word='study', vector=DenseVector([-0.1102, -0.1107, 0.1408])), Row(word='compare', vector=DenseVector([0.0113, 0.0292, 0.0105])), Row(word='like', vector=DenseVector([0.02, 0.0907, -0.029])), Row(word='other', vector=DenseVector([-0.1051, -0.0556, -0.1351])), Row(word='of', vector=DenseVector([-0.116, 0.0714, -0.0658])), Row(word='states', vector=DenseVector([-0.0352, -0.1017, 0.1529])), Row(word='containing', vector=DenseVector([0.0293, -0.0299, 0.1167])), Row(word='and', vector=DenseVector([-0.2291, 0.1073, 0.0488])), Row(word='primary', vector=DenseVector([-0.0953, -0.1253, 0.163])), Row(word='one', vector=DenseVector([0.0945, -0.1683, -0.1001])), Row(word='free', vector=DenseVector([0.0391, -0.0878, 0.1411])), Row(word='classical', vector=DenseVector([-0.1068, -0.0918, -0.0272])), Row(word='beyond', vector=DenseVector([0.0101, -0.0826, -0.0076])), Row(word='quantum', vector=DenseVector([-0.1037, 0.1073, -0.057])), Row(word='holding', vector=DenseVector([-0.1675, 0.1564, 0.0863])), Row(word='refractive', vector=DenseVector([0.0946, -0.0177, -0.1027])), Row(word='investigate', vector=DenseVector([-0.0113, 0.0133, -0.0212])), Row(word='view', vector=DenseVector([0.1611, 0.0806, 0.0597])), Row(word='level', vector=DenseVector([-0.1373, 0.0276, -0.009])), Row(word='investigations.', vector=DenseVector([0.0241, 0.1458, 0.1208])), Row(word='the', vector=DenseVector([-0.4258, 0.1632, 0.086])), Row(word='physics,', vector=DenseVector([-0.0344, -0.0812, -0.1427])), Row(word='helmholtz-like', vector=DenseVector([-0.0223, 0.1452, -0.1036])), Row(word='physical', vector=DenseVector([0.0812, 0.0667, 0.0974])), Row(word='those', vector=DenseVector([0.0967, 0.0594, 0.0127])), Row(word='only', vector=DenseVector([0.037, -0.0651, 0.1358]))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCbv_TB_SeqZ",
        "outputId": "a6eaefda-7673-452c-9b81-7e96f91d6193"
      },
      "source": [
        "# Show the synonyms and cosine similarity of the word in input data\r\n",
        "synonyms = model.findSynonyms(\"science\", 10)\r\n",
        "synonyms.show(10)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+------------------+\n",
            "|       word|        similarity|\n",
            "+-----------+------------------+\n",
            "| describing|0.9997835755348206|\n",
            "|      prior|0.9970971345901489|\n",
            "|       call| 0.979093074798584|\n",
            "|      don’t|0.9779413342475891|\n",
            "|       term|0.9775862097740173|\n",
            "|investigate| 0.971872091293335|\n",
            "| omniscient|0.9716155529022217|\n",
            "|literature.|0.9690399765968323|\n",
            "|      case.|0.9689924120903015|\n",
            "|     field,|0.9689012765884399|\n",
            "+-----------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtdspYI3Qg1o"
      },
      "source": [
        "#closing the spark sessions\r\n",
        "spark.stop()\r\n",
        "spark2.stop()\r\n",
        "spark3.stop()\r\n",
        "spark4.stop()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}